-------------------------------------------------------------------------------------

* 사인코사인 인코딩

방향의 경우 사인코사인 변환을 통해서 인코딩 할 수 있다.

이때 사인, 코사인 두 피처를 같이 사용해야 정확한 좌표를 지정할 수 있다.

사인과 코사인 각각이 유의미한 지표가 아닐지라도, 모델이 학습시에 성능이 높아진다면 중요한 지표로 사용될 수 있다.

-------------------------------------------------------------------------------------

* 타겟 기반 재범주화

타겟를 기준으로 만든 피처를 다시 타겟 예측에 사용한다면 과적합 가능성 있음.

-> 가장 좋은 건 train → validation으로 나누고
train 데이터만 이용해서 게재일_구간을 정의,
그리고 validation에도 같은 기준으로 적용

train과 valid가 명확히 분리돼 있더라도,
우리가 지금 **train 전체를 보고 만든 통계 기반의 범주(게재일_구간)**를
다시 train 학습 피처로 넣는다면,
과적합 또는 데이터 누수 위험이 있을 수 있음

이걸 학습 데이터(train)의 정답 레이블을 보고 정의했기 때문에,
그걸 다시 train에 쓰는 순간에는 정보가 유출

-> 교차검증을 통한 평가를 통해 높은 평균과 낮은 표준편차를 유지한다면, 일관된 성능 기여가 있는 것으로 판단함.

* 타겟 기반 피처

-> 주어진 피처들만으로는 설명력이 충분하지 않기 때문에 타겟과 직접 연관된 외부적인 단서를 피처로 삼으려는 시도.

그렇게 바람직한 상황은 아니라고 생각. 모델링의 한계

-> 결과에서 원인을 거꾸로 만드는 행위

EDA나 내부 분석 목적으로는 매우 유용함 (힌트, 시각화 등)

**** 모델링 단계에서는 과적합 우려로 사용이 제한적이지만, 인사이트를 얻는 데에는 매우 유용한 피처

-------------------------------------------------------------------------------------

* 시계열 기반 데이터에 대한 교차검증을 할때는 TimeSeriesSplit

-------------------------------------------------------------------------------------

* 게재일 피처(부동산 허위매물 예측)

타겟을 기반으로 구간을 나눴을때, 허위매물의 비율이 명확하게 나눠지는 결과가 나왔다.
하지만 이는 타겟 기반이기 때문에 과적합의 가능성이 존재해 사용할 수 없다.

년월을 카테고리화 하여 사용하는 것은 카테고리의 수가 너무 많기 때문에 모델의 학습에 있어 문제가 될 수 있다.

분기/ 계절을 통한 재범주화도 기간이 맞아 떨어지지 않아서 사용할 수 없다.

실제 현상을 반영할 수 있다면, 누수가 없다는 전제 하에 사용해도 된다. -> 현실을 기반으로 설명 가능한 피처이기 때문에 예측력 뿐만 아니라 해석력도 챙길 수 있다.

확인
분석 결과 vs 실제 정책 비교 
데이터 설명서/문서 확인 - 대회 측에서 "실제 데이터를 기반으로 했다"고 명시했는지
이상치/패턴 관찰 - 어떤 구간이 너무 인위적으로 꺾이거나, 비현실적 트렌드가 있는지 체크

-------------------------------------------------------------------------------------

* 결측치 보간

- 결측여부와 결측치

결측여부와 다른 피처간의 관계를 통해 결측치가 어떻게 발생했는지 파악, 결측 패턴을 설명. -> 분류 피처로 활용.

하지만 결측값을 채울때는 피처 자체가 다른 피처와 어떤 관계를 갖는지 파악하는게 중요.

탐색한 결측여부 피처가 방수와 연관이 있고, 총주차대수 결측여부와 연관이 있다.
-> 결측이 어떤식으로 발생했는지 결측의 패턴을 설명할수 있다.
이를 활용해서 결측치를 채울수는 없다. 

결측치를 채우려면 전용면적 피처와 다른 피처간의 관계를 활용해서 채워야 한다. 결측여부 피처는 후에 판단을 통해 타겟 예측에 활용할지 말지를 선택하면 된다.

-------------------------------------------------------------------------------------

* 보간 방법

결측치의 수가 매우 적은 경우 단순보간을 통해 최빈값, 중앙값을 사용하면 된다.

만약 결측피처와 연관있는 피처들이 대부분 범주형 피처인 경우, 그룹화한 뒤 최빈값, 중앙값을 사용하면 된다.

랜덤 포레스트의 경우 비선형 관계도 학습이 가능하고 예측의 기여도가 낮으면 자동으로 무시함. 모든 피처와의 상호작용 효과를 기대할 수 있다.
이때 피처에 결측이 없어야 하며, 카테고리 수가 많은 경우 추려서 원핫인코딩을 한 뒤 보간에 사용.

하지만 그룹기반의 보간은 해석력이 좋고 빠드라는 장점이 있다.
연속형 피처와 높은 연관이 있는 경우에는 모델 기반 보간이 더 유리.

-------------------------------------------------------------------------------------

*** RFE를 위한 피처 중요도

Model Feature Importance, Permutation Importance, SHAP 등 다양한 피처 중요도 평가 방식이 존재

그중에 SHAP은 타겟에 대한 영향력을 플랏으로 정량/정석적 분석이 가능 하기 때문에 선호됨

SHAP을 통한 피처제거가 좋은 이유

개별 예측에 대한 기여도를 정량적 + 정성적으로 해석이 가능. 다른 Feature Importance보다 직관적이며 설명력이 높음

-------------------------------------------------------------------------------------

%matplotlib inline은 노트북의 맨 위

-------------------------------------------------------------------------------------

* Threshold 조정

불균형한 데이터의 경우 0.5가 아닐 수 있음. 기준에 따라서 조정을 해야함. 0.1에서 0.9정도 사이에서 0.05간격으로 조정하며 실험을 통해 최적의 Threshold값을 선택.

F1 스코어와 정밀도, 재현율이 균형적인 지점을 찾아야 한다.


-------------------------------------------------------------------------------------

* count, bar, hist, box 비교(맨날 햇갈림)

count - 범주별 빈도수를 막대로 표시(개수)

bar - 범주형 데이터 카테고리 별 집계값(평균, 합 등)을 막대 형태로 표시

hist - 연속형 변수의 분포를 표시. 막대형태지만 전체적인 그래프의 형태가 나타남.(kde)

box - 연속형 변수의 분포와 이상치를 요약해서 보여줌.

-------------------------------------------------------------------------------------

* EDA 과정에서 타겟과 연관이 없다고 판단되는 피처의 제거가 가능한지

도메인 지식상 무의미한 변수, 누락률이 매우 높고, 결측 자체에도 특별한 의미가 없는 경우는 가능.

하지만 비선형 관계 또는 복잡한 상호작용이 포함되어 있을수 있기때문에 제거하면 안됨. 나중에 피처 엔지이니어링에서 활용될 수 있음.

-> 따라서 보류해두고 제거는 모델링 단계에서 진행하는게 좋음.

-------------------------------------------------------------------------------------

* 변환을 통한 정규성 확인

*** 정규성은 모델의 학습에 도움을 주는 요소일뿐.
*** 타겟과 해당 변수와의 관계(해석력, 설명력)가 더 중요하다.

왜도 - 분포의 비대칭성(얼마나 한쪽으로 치우쳤는지) 

    0에 가까울수록 좋음.
    
첨도 - 분표의 뾰족하거나 평평한 정도

    3에 가까울수록 정규분포에 가까움
    
-> 왜도가 0.5이하여도 첨도가 너무 높으면 극단값의 영향이 큼. 정규성을 만족한다고 보기 어려움.

변환은 타겟과 해당 변수간의 관계를 해치치 않는 선에서 시행해야 한다.

변환 전 후의 타겟과 변수간의 상관계수의 변화(피어슨/스피어만), 시각적 패턴 등을 통해 확인해야 한다.

    분류문제일때는 박스와 바이올린, 회귀일때는 산점도 등.
    
위의 내용들이 보장되는 선 내에서 왜도는 0에 가깝게, 첨도는 3에 가깝도록 변환을 진행.

실무에서는 첨도가 4이상일때, 왜도가 0.5이상일때 변환을 고려한다.

*** 가장 중요한 것은 타겟과의 관계가 더 명확해 졌는지/ 유지되고있는지

-------------------------------------------------------------------------------------

* Q-Q plot

정규분포를 잘 따르는 데이터라면 대부분의 점들이 대각선에 잘 맞춰져 있음.

S자 형태를 띄면서 대각선의 왼쪽 위에 분포한다면 양의 왜도, 아래에 분포한다면 음의 왜도이며 꼬리의 길이가 왜도의 크기이다.

또한 얼마나 벗어났는지는 첨도를 의미하며 많이 벗어날수록 첨도가 커진다.

-------------------------------------------------------------------------------------

* 타겟이 0.0에서 1.0 사이 0.1 간격으로 존재할때.

연속형 / 카테고리형으로 모두 간주할 수 있음.

단, 실제로 확률적 의미를 가진다면 회귀(수치를 보존하는게 중요)

성공확률값이 라벨처럼 평가되어있다면 분류(서열만 있고 간격이 무의미할 수 있음)

-> 대회의 경우 명시되어있음.

기업성공확률 : 회귀 / Weighted MAE로 명시되어있기 때문에 연속형이 맞음.

-------------------------------------------------------------------------------------

* 군집구조 파악

PCA를 통한 군집구조 파악은 선형적 구조를 중심으로 하기 때문에, 직관적이지만 비선형 구조는 파악하기 어려움.

-> 경계가 흐릿한 군집이 존재할때

t-SNE (t-Distributed Stochastic Neighbor Embedding) - 비선형 차원 축소 기법

    단점: 거리 왜곡 있음 → 시각화 전용 / 모델 입력 X
    군집이 정말 존재하는 것 같긴 한데, PCA로 안 보일 때
    
UMAP (Uniform Manifold Approximation and Projection)
    
    비선형 구조 + 글로벌 구조도 어느 정도 보존

Hierarchical Clustering + 덴드로그램

    군집을 병합해가면서 계층적으로 클러스터를 만들고, 덴드로그램으로 시각화
    군집 경계가 점진적으로 형성되는지 확인 가능

-------------------------------------------------------------------------------------

* 군집분석

이상치, 결측치가 정리된 상태여야 안정적

스케일 차이가 큰 변수들에 대해서 스케일링이 필요함.

- 시각화 및 군집 파악을 위한 변환

    PCA : 분산을 보존하며 선형구조를 통해 저차원으로 축소
    
    t-SNE : 비선형 구조 시각화에 탁월, 군집 경계 뚜렷
    UMAP : t-SNE보다 빠르고 군집 분리 잘 됨
    
    -> scatterplot으로 시각화 하고 나중에 클러스터 라벨 부여
    
- 군집 알고리즘 적용

    KMeans : 중심기반, 구형군집. 비교적 균일한 분포일때 사용
    DBSCAN : 밀도기반, 이상치 탐지 포함. 복잡한 모양, 이상치 포함일때 사용
    
- 군집 수 결정

    엘보우 방법, 실루엣 계수
    eps 튜닝

-------------------------------------------------------------------------------------

* SHAP 기반 피처 셀렉션 및 버전 비교 분석 정리

1. 버전별 피처 구성

2. 모델링 및 평가

    모델: LightGBM  
    평가 지표: F1 Score (예측 성능)
    해석 지표: SHAP Top 5 피처의 평균 중요도 비율 (해석력)
    
3. SHAP 분석을 통해 확인한 것
    
    예측 성능이 높더라도 모델이 특정 피처에 과하게 의존하거나 해석 가능한 피처가 없을 경우 실무 적용이 어려움  
    반대로 SHAP 평균 중요도가 높으면서 성능도 높은 조합은 예측력과 해석력 모두 확보된 이상적인 구조
    
    
    버전별 평균 F1 Score + 평균 SHAP 중요도 (막대 + 점선 그래프)
    → 성능과 해석력의 균형 여부 시각적으로 판단 가능
    
    SHAP Dependence Plot (상위 3개 피처)
    → 주요 피처가 예측에 어떤 방식으로 영향을 미치는지 확인

    Box + Stripplot (대표 피처별 보증금 분포)
    → 피처별로 타겟 변수와의 연관성이 시각적으로 드러나는지 확인

주의 사항

    SHAP 평균이 높아도 F1이 낮다면
    → 해석은 되지만 성능이 약한 모델, 신호가 약하거나 과적합 가능성 있음
    
    SHAP 평균이 낮고 F1도 낮다면
    → 모델이 학습을 제대로 못하고 있음, 피처 구조 자체를 재설계해야 함
    
    가장 좋은 조합은:  
    → F1 Score 높음 + SHAP 평균 높음 + SHAP Top 피처가 해석 가능한 경우
    
    SHAP이 지표만 높은 것이 아니라, 의미 있는 피처에 집중되고 있는지 확인이 필요

-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------






-------------------------------------------------------------------------------------
