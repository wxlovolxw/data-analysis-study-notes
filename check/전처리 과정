전처리 과정

---------------------------------------------------------------------------------------

(1) 데이터 탐색 (EDA) -> *** 데이터의 상태를 점검하고, 전처리 방향을 결정하는 과정

먼저 데이터의 분포, 이상치, 결측치, 클래스 불균형 등을 확인

데이터의 수치형/범주형 특성을 파악해야 함

- 어떤 타입의 데이터가 존재하며, 값들은 어떻게 분포하는지 등을 파악. 이때 적절한 플랏을 통해서 올바른 형태의 데이터들에 맞게 확인하는 것이 중요. 확인한 데이터들의 형태를 통해서 분석의 순서 및 방향성을 설정. 결측치가 어떤형태로 얼마나 존재하는지, 이상치의 분포는 어떠한지, 많은지 적은지 등등을 파악함으로써 향후 분석이 어떻게 진행될지를 확인하는 단계

- 이상치와 결측치를 확인해야 하는 이유

    데이터의 품질 향상, 전처리 전략 수립(결측치 패턴에 따라 삭제, 대체, 모델기반 대체, 이상치를 비즈니스적으로 의미 있는지 검토 후 처리 방향 결정)
    모델의 성능 영향 방지(이상치를 처리하지 않으면 모델이 극단적인 값에 영향을 받을 가능성이 높음, 결측치도 모델 학습 시 오류가 발생할 수 있음.)

데이터 타입의 확인 - 숫자형, 범주형, 문자열, 날짜 타입들의 데이터 확인 + 정규성을 만족하는지 여부(모수검정을 사용할지 비모수 검정을 사용할지 + 이상치 탐지방법에 영향)

기술통계 확인 - 데이터의 분포(평균, 중앙값, 표준편차, 첨도, 왜도), 각 변수의 타입, 결측치 여부 확인

시각화를 활용한 탐색

    연속형 변수 - 히스토그램, 박스플랏, KDE, 바이올린플랏
    범주형 변수 - 바 플롯, 파이 차트
    
    -> 연속형 변수라면 정규성까지 확인.(정규성을 따르는지 여부에 따라 검증법이 다름)
    
결측치 분석 - 결측치의 개수 확인. ->  비율 등을 통해서 어떤식으로 처리할 것인지 방식 고려

    결측치 패턴의 시각화를 통해 특정 컬럼끼리 결측치가 발생하는지, 랜덤한지, 패턴이 존재하는지
    
    결측여부와 다른 피처들간의 관계 분석
    *** MCAR, MAR, MNAR -> 어떤형태의 결측치인지를 통해 결측치 처리 방법이 달라짐.
    또한 결측이 어떤식으로 발생했을 것인지에 대한 인사이트 제안.
    하지만 결측여부가 직접적으로 결측값에 대한 정보를 주지는 않음.
    
이상치 탐색 - IQR 분석, Z-score분석, Box plot, Violin plot을 통해 시각적으로 확인
    
    군집의 형태를 띄는지 여부를 먼저 확인.
    산점도와 PCA(주성분분석)을 통해 플랏으로 정석적인 분석.
    KMeans 클러스터의 엘보우, 실루엣 계수 들을 통한 정략적인 분석을 통해 군집 여부 확인 후,
    군집이 있다면 DBscan을 통한 이상치 탐색 및 제거를 진행.
    
    군집이 없다면 개별 피처간 탐색 진행.
    정규성을 따르지 않는다면 IQR(비모수), 따른다면 Z-score(모수)를 통해 이상치 탐지 및 제거.
    
    제거하기 전후 왜도를 비교.
    왜도가 0.5보다 낮다면 변환 불필요, 높다면 왜도를 최소화하는 방향으로 변환 시행.
    
    변환 후에 정규성을 따르는지 검정 및 플랏 확인.

상관관계 분석 -> 수치형 변수간의 관계 파악, 주요 변수 파악

    히트맵을 통해 변수간의 관계 시각화
    pairplot을 통해 산점도 행렬 확인
    
    다중공선성이 있는 경우, 변수를 줄일 방법을 고려. *** 비선형 관계가 존재할 가능성이 있는지 탐색
    
    df.corr(method='pearson')   # 선형 상관관계 측정
    df.corr(method='spearman')  # 비선형 상관관계까지 측정
    df.corr(method='kendall')   # 비선형 관계 포함한 상관 분석
    
    선형 회귀를 통해 다항식 변환을 고려.
    
    *** 트리기반 모델의 경우에는 공선성의 영향을 덜 받기 때문에 괜찮음. 하지만 해석이 목적이라면 공선성을 최대한 줄이는 것이 좋음.

범주형 변수 분석 - 범주형 데이터의 개수 확인.

    countplot을 통해 시각화. -> 확인 후 그룹화 고려
    
    범주형 데이터가 너무 많을 경우 모델링 성능에 영향을 줄 수 있음
    
    타겟과 범주형 변수 간의 관계도 분석 - *** crosstab() 
    
타겟 변수 탐색 - *** 타겟의 불균형 여부 확인 -> 불균형이 크다면 SMOTE 등 활용을 고려

    타겟과 주요 변수 간의 관계 분석, 시각화 - Box, Bar 플랏 등
    
    타겟에 이상치가 포함될 가능성이 있는지 점검
    
/////////////////////////////
    
*** 데이터 타입별로 독립성 검증법

1) 범주형 - 연속형

    (1) 범주형의 카테고리수가 3개 이상 - 연속형이 정규성 만족 : ANOVA(일원 분산 분석)
    
        from scipy.stats import f_oneway

        f_stat, p_value = f_oneway(
            train[train["범주형_변수"] == "A"]["연속형_변수"],
            train[train["범주형_변수"] == "B"]["연속형_변수"],
            train[train["범주형_변수"] == "C"]["연속형_변수"]
        )
        print(f"ANOVA 결과: F-statistic={f_stat:.4f}, p-value={p_value:.4f}")
        
        p값이 0.05보다 작다면, 범주형 변수에 따라서 연속형 변수의 값이 유의미하게 다름
    
    (2) 범주형의 카테고리수가 3개 이상 - 연속형이 정규성 만족X : Kruskal-Wallis 검정
    
        from scipy.stats import kruskal

        h_stat, p_value = kruskal(
            train[train["범주형_변수"] == "A"]["연속형_변수"],
            train[train["범주형_변수"] == "B"]["연속형_변수"],
            train[train["범주형_변수"] == "C"]["연속형_변수"]
        )
        print(f"Kruskal-Wallis 검정 결과: H-statistic={h_stat:.4f}, p-value={p_value:.4f}")
        
        p값이 0.05보다 작다면, 범주형 변수에 따라서 연속형 변수의 값이 유의미하게 다름
    
    (3) 범주형의 카테고리수가 2개 - 연속형이 정규성 만족 : 독립 표본 t-test
    
        from scipy.stats import ttest_ind

        group1 = train[train["범주형_변수"] == "A"]["연속형_변수"]
        group2 = train[train["범주형_변수"] == "B"]["연속형_변수"]

        t_stat, p_value = ttest_ind(group1, group2, equal_var=False)
        print(f"T-test 결과: t-statistic={t_stat:.4f}, p-value={p_value:.4f}")
    
    (4) 범주형의 카테고리수가 2개 - 연속형이 정규성 만족X : Mann-Whitney U검정
    
        from scipy.stats import mannwhitneyu

        group1 = train[train["범주형_변수"] == "A"]["연속형_변수"]
        group2 = train[train["범주형_변수"] == "B"]["연속형_변수"]

        u_stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')
        print(f"Mann-Whitney U 검정 결과: U-statistic={u_stat:.4f}, p-value={p_value:.4f}")


2) 연속형 - 연속형

    (1) 정규성 만족 : 피어슨 상관계수
    
        from scipy.stats import pearsonr

        corr, p_value = pearsonr(train["연속형_변수1"], train["연속형_변수2"])
        print(f"Pearson 상관계수: {corr:.4f}, p-value: {p_value:.4f}")
    
    (2) 정규성 만족X : 스피어만 상관계수
    
        from scipy.stats import spearmanr

        corr, p_value = spearmanr(train["연속형_변수1"], train["연속형_변수2"])
        print(f"Spearman 상관계수: {corr:.4f}, p-value: {p_value:.4f}")
    
    (3) 정규성이 의심되거나 이상치가 많음 : 켄달 상관계수 
    
        from scipy.stats import kendalltau

        corr, p_value = kendalltau(train["연속형_변수1"], train["연속형_변수2"])
        print(f"Kendall 상관계수: {corr:.4f}, p-value: {p_value:.4f}")
        
3) 범주형 - 범주형

    카이제곱 검정
    
        from scipy.stats import chi2_contingency

        # 교차표 생성
        contingency_table = pd.crosstab(train["범주형변수1"], train["범주형변수2"])

        # 카이제곱 검정 수행
        chi2, p_value, dof, expected = chi2_contingency(contingency_table)
        print(f"카이제곱 검정 결과: Chi2-statistic={chi2:.4f}, p-value={p_value:.4f}")

/////////////////////////////

---------------------------------------------------------------------------------------

(2) 결측치 처리 (Missing Values)

결측치란 다양한 이유로 측정되지 않는 값을 뜻. 여기에 속하는 다양한 이유에는 애초에 기입을 거부해서, 혹은 오류로 인해 값이 제거되거나, 애초에 없는 값이거나 등등 다양한 이유가 속함. 이런 결측된 값들로 인해서 부정확하게 모델이 학습될 수 있기 때문에 데이터 분석에 있어서 결측값의 처리는 매우 중요한 과정. 결측값의 처리에는 다양한 방법이 있는데 결측치가 있는 행 자체를 제거하거나, 다른 값으로 대체하는 등의 방식이 있음. 이런 방법을 적용시키는 데에는 결측치들의 기원과 비율, 패턴 등을 우선적으로 파악하는 것이 우선적. EDA에서 진행헀던 결측치 탐색의 결과를 가지고 결측치들을 적절히 보완해야함.

주요 탐색할 사항들

    - 결측치의 패턴과 분포 -> 특정 범위에서 집중적으로 발생하는지 확인
    
    - 결측치 간의 상관관계 -> 다른 피처의 결측치와 연관 있는지 확인.(피처의 결측치가 피처의 특정 값들이라면?)
    
    - 결측치 비율이 특정 범주에서 높은지 확인
    ex) groupby를 통해 특정 범주에서 집중되는지 확인
    
    - 결측치가 타겟 변수와 연관이 있는지 -> 연관성이 높다면 결측 여부를 피처로 유지
    
    - 결측치가 특정 시점에서만 발생하는지 확인(시간의존성)
    
    - 특정 변수와 결측치 간의 관계를 시각적으로 확인
    ex) 특정 연령대에서만 결측치가 많다 -> 단순 무작위 결측이 아님


결측치의 종류 

    - 완전 무작위 결측 : 패턴 없이 랜덤하게 발생한 결측치
    ex) 센서 오류, 설문 응답자의 랜덤한 무응답
    
    - 조건부 무작위 결측 : 다른 변수와 관련이 있지만, 결측이 발생한 변수 자체와는 관련이 없음
    ex) 여성 응답자가 연봉 정보 기입 거부(성별과 관련 있지만, 연봉과는 직접적 관계가 없음)
    
    - 무작위가 아닌 결측 : 결측치가 해당 변수 자체와 관련이 있음
    ex) 고소득자가 연봉 기입 거부
    
처리 방법

    5-10이면 삭제가능. 10-30이면 대체, 30-50이면 상황에 따라 다름-> 중요면 유지, 아니면 삭제 고려. 너무 높으면 적극 삭제 고려

    - 삭제 : 비율이 적고 랜덤하게 발생한 경우.
    
    - 단순 대체 : 최빈값, 평균, 중앙값 등으로 결측값을 채움
    
    수치형 변수는 평균, 중앙값
    범주형 변수는 최빈값

    - 고급 대체 기법
    
        *** KNN - 비슷한 데이터 샘플을 찾아 대체(KNNImputer)
        랜덤 포레스트 회귀 - 결측값을 예측하여 채움
        
    - 결측 자체를 변수화 -> 걸측치 유무를 만들고, 원래 변수를 대체하면 강한 상관관계를 갖게 됨.(다중 공선성 문제가 발생할 가능성이 높음)
    
    타겟이 카테고리일때 카이검정으로
    타겟이 수치형일때 독립표본t검정으로(ANOVA)
    
    *** 새로운 카테고리를 추가하는 방식을 사용. 결측치 유무 피처간의 상관관계가 줄어듬.
    
결측치가 많은 경우 삭제 or 대체(평균, 중앙값, 최빈값, 모델 예측 등)
범주형 결측치는 "Unknown", "Other" 같은 새로운 값으로 대체하는 경우도 있음
특수한 경우:
결측치 자체가 의미를 갖는 경우: 결측 여부를 파생 변수로 만들고, 다른 방법으로 대체
비즈니스 도메인 지식 활용: 특정 규칙에 따라 채우기

---------------------------------------------------------------------------------------

(3) 이상치 처리 (Outlier Handling) - 데이터의 일반적인 패턴에서 벗어난 극단적인 값들의 처리

- 분석 및 모델링 과정에서 통계적인 값들에 영향을 줄 수 있음. 제거할지 변환할지 결정하는 과정이 매우 중요

*** 이상치를 처리하기 이전에, 이상치 자체가 중요한 의미를 가질 수 있음.

-> 따라서 이상치가 타겟과 유의미한 관계가 있는지 파악해야함.
또한 패턴이 있는지 없는지를 통해서 랜덤한 이상치면 제거, 패턴이 있으면 변환을 고려해야함.

이상치를 먼저 제거한 후에 변환을 진행하며, 항상 왜도를 확인해야 한다.  
왜도가 너무 크다면 변환을 진행해야 하지만, 과도한 변환은 해석성과 안정성을 해치기 때문에 왜도가 0.5보다 작다면 일반적으로 변환을 진행하지 않는다.

너무 범위를 벗어나는 값들을 제거하거나 변환을 해주는 과정. 주로 z-score나 IQR 방식, DBSCAN을 통해서 이상치들을 탐지하고, 범위를 벗어나는 값들을 없애기 위해 제거하거나 적절한 변환을 해줌. 주로 왜도를 줄이는 방식의 변환을 해주는데 주로 로그변환, 제곱근변환, box-cox변환이 있음.

*** 변환해야할 단위가 있다면 먼저 우선적으로 한 뒤, 로그나 박스콕스 변환을 사용.

탐지 - 군집형태인지 먼저 확인 후, 군집형태를 띄면 DBSCAN을 사용하고, 아니라면 개별피처들에 대해서 IQR 혹은 Z-score를 사용하면 된다.
    
    Z-score : 데이터가 정규분포를 따른다고 가정했을떄, 평균에서 얼마나 떨어져 있는지 확인.
    
        정규분포가 아닐 때 잘못 탐지할 수 있음.
        극단적인 이상치가 존재할시 평균과 표준편차가 왜곡될 가능성이 있음.
        
    IQR : 데이터의 중간 50% 범위를 기준으로 이상치를 판단
    
        사분위 범위를 벗어나는 값을 이상치로 탐색
        정규분포를 따르지 않는 데이터에서도 적용 가능
        극단적인 이상치의 영향을 덜 받음
        
    DBSCAN : 군집 기반 이상치 탐색 기법. 밀도가 낮은 데이터를 이상치로 판단
    
        비정형 데이터(군집형태)에 적합
        선형적인 기준보다 유연한 이상치 탐지 가능
        데이터가 특정 패턴을 가질때 효과적
        
    * 이상치 탐지 방법별로 이상치의 비율이 달라지기 떄문에 분포를 통해서 적절한 탐지법을 사용하는 것이 중요하다.
    
    *** 군집 형태를 띄는지 확인하는 방법
    
        산점도 활용 - 육안으로 확인하는 직관적인 방법 -> pair plot을 사용하면 한번에 여러 변수 조합을 산점도로 확인 가능하다.
        
        PCA(주성분분석) - 주성분분석을 통해 차원 축소 후 시각화를 통해 확인
        
        KMeans 클러스터 개수 - 군집 형태를 가진다면, 특정 클러스터 개수에서 오류(WCSS)가 급격히 줄어드는 패턴이 나타남
        
        실루엣 계수(Silhouette Score) 활용 - 군집 형태가 있으면 실루엣 계수가 높아짐(1에 가까울수록)
            
            0.5 이상이면 군집 구조가 명확, 아니면 군집이 없는 데이터일 가능성이 높음
            
        DBSCAN 자체를 먼저 실행
        
/////////////////////////////

*** DBSCAN의 eps를 데이터에 맞게 자동으로 설정하는 방법
     
from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# k-최근접 이웃(k-NN)으로 최적의 eps 찾기
neigh = NearestNeighbors(n_neighbors=5)
neigh.fit(train[["Feature1", "Feature2"]])
distances, indices = neigh.kneighbors(train[["Feature1", "Feature2"]])

# 거리 그래프 그리기
distances = np.sort(distances[:, 4], axis=0)  # 4번째 이웃 거리 사용
plt.plot(distances)
plt.xlabel("Data Points sorted by distance")
plt.ylabel("Epsilon (eps) distance")
plt.title("Optimal eps selection using k-NN distance")
plt.show()

그래프에서 급격한 변화가 있는 지점이 eps로 적절한 값

min_samples = 데이터 차원(D) * 2 정도로 설정

*** 실제 군집인지 확인하는 방법

 - 실루엣 점수 (Silhouette Score) - 군집이 실제 유의미한지 확인하는 지표

from sklearn.metrics import silhouette_score

sil_score = silhouette_score(train[["Feature1", "Feature2"]], train["DBSCAN_cluster"])
print(f"Silhouette Score: {sil_score}")

0.5이상일시 군집이 명확

 - 군집 개수 확인 - DBSCAN 결과에서 군집 개수가 극단적으로 많거나 적다면 파라미터가 잘못된 것일 가능성이 높음
 
print(train["DBSCAN_cluster"].value_counts())

/////////////////////////////
        
처리

    제거 : 이상치가 많지 않고 랜덤하게 발생했을때 유용. 데이터 손실이 큰 경우 제거하면 안됨.
    
    변환 : 이상치를 바로 제거하지 않고, 변환을 통해 정규성을 유지
    
        로그변환 - 데이터의 분포가 오른쪽으로 긴 꼬리를 가지는 경우(값이 매우 클때)
        값이 급격하게 증가하는 데이터에 적합.
        0이하의 값이 존재하면 적용 불가능
        ex) 소득, 주택가격, 인구, SNS 팔로워 수
        
        제곱근변환 - 양의 값에서 큰 값을 완화. 값이 작을때도 상대적으로 효과적.
        이상치를 줄이는 효과는 있지만 로그변환보다는 덜 강력
        음수일때는 변환 불가능. 급격한 변화가 필요하지 않을때 대안으로 사용
        ex) 포인트 수, 거리, 키, 무게
        
        Box-cox변환 - 로그와 비슷하지만, 최적의 람다값을 찾아 변환
        정규분포를 따르도록 조정하는데 효과적
        로그변환, 제곱근변환보다 일반적인 변환 방법
        음수값이 있으면 적용 불가능
        
스케일링 - 변수간의 크기 차이를 조정하여 모델이 더 잘 학습할 수 있도록 만드는 과정

    표준화(StandardScaler) : 평균 0, 표준편차 1로 변환
    정규분포를 잘 따르는 데이터에서 가장 효과적
    선형 모델이나 PCA 등 차원 축소 알고리즘에서 효과적
    이상치로 인해서 평균과 표준편차가 왜곡됨
    
    *** 모든 변환을 동일한 스케일로 변환.(변환 전에 특정 변수에 더 큰 가중치를 부여할 수 있음. ex) 키 170 몸무게 70이면 키에 더 큰 가중치)
    스케일이 맞지 않으면 성능이 떨어지는 모델이 있음 - 선형모델(로지스틱회귀, 선형회귀), PCA, SVM
    모델이 더 빠르게 수렴할 수 있도록 도와줌 - 딥러닝, 신경망 모델에서는 거의 필수적
    
    MinMaxScaler : 데이터를 0~1의 범위로 변환하여 모든 값이 같은 범위 내에 있도록 조정
    정규분포가 아닐때도 사용 가능
    이상치가 있으면 데이터 범위가 왜곡 될 수 있음.
    
    RobustScaler : 중앙값과 IQR를 활용하여 스케일링(5% 이상일때 StandardScaler보다 성능이 좋아짐)
    이상치가 있는 경우에도 강건한 변환을 제공
    오히려 이상치가 적으면 효과가 크지 않을 수도 있음.
    
    *** LightBGM이라던지 XGBoost같은 트리기반의 경우 스케일링이 필요하지 않음.
    
단위 변환 - 같은 단위인 경우 같은 방식으로 변환 하는 것이 중요.

    변환만으로 충분하면 스케일링이 필요하지 않다. 정규화가 필요하다면 추가 적용
    
*** 변환과 스케일링의 차이

    변환은 데이터의 분포를 조정하는 과정 : 데이터의 분포를 정규분포와 같은 형태로 만듬
    
    스케일링은 데이터의 크기(범위)를 조정하는 과정
    
    -> 일반적으로 변환 후 스케일링을 진행
    

이상치는 IQR(사분위 범위), Z-score, DBSCAN 같은 방법으로 탐지 후 제거 or 수정
특수한 경우:
이상치가 비즈니스적으로 의미를 가질 때: 그대로 유지
이상치가 타겟 변수와 강한 상관관계를 가질 때: 제거 대신 별도 그룹으로 분류 가능

---------------------------------------------------------------------------------------

(4) 범주형 변수 처리 (Categorical Feature Handling)

카테고리화 기준

    타겟 변수와의 관계를 고려하여 상관성이 높은 범주를 우선 유지하여 카테고리수 선정
    타겟과 유사한 패턴을 가진 범주끼리 그룹화 하는 것이 효과적
    
카테고리 인코딩 방법 고려 필요

    범주형 변수를 처리한 후 최종적으로 숫자로 변환하는 과정이 필요함
    
    원핫 인코딩 : 카테고리 개수가 적을때 적합(3~5)
    빈도 인코딩 : 데이터 개수가 많을때 사용 가능
    타겟 인코딩 : 타겟 값과 관계를 반영할 때 유용하지만 과적합 위험 존재
    레이블 인코딩 : 순서가 있는 변수일때 사용. ex) 요일, 학력수준, 등급, 직급
    
    *** 순서가 중요한 피처인경우 레이블, 카테고리 수가 적으면 원핫, 카테고리 수가 많으면 빈도를 사용하자.
    
/////////////////////////////
    
*** 빈도 인코딩 - 각 범주형 변수의 등장 빈도를 숫자로 변환하는 방법.
        
    각 카테고리가 데이터에서 얼마나 자주 등장하는지를 숫자로 변환하여 학습
    카테고리 개수가 많을 때 차원의 증가를 방지 -> 원핫을 사용하면 차원수가 너무 커짐
    모델이 특정 카테고리의 등장 빈도를 학습할 수 있도록 함
    트리기반 모델에서 효과적
        
    value_counts(normalize=True)를 사용
    
    -> 단점 : 선형 모델에서는 효과가 떨어질 수 있다 -> 스케일링을 적용하여 해결 가능
    
    높은 빈도수로 인해서 더 큰 값을 갖는다고 중요한 값이라는 의미는 아님 
    빈도값이 적다고 해서 타겟과의 관계가 반드시 적은 것은 아님
    
    -> 스케일링을 통해서 특정 카테고리의 빈도가 너무 높거나 낮아서 선형 모델이 이를 과하게 반영하는 문제를 완화할 수 있음.
    
    타겟과의 직접적인 관계를 반영하고 싶으면 타겟 인코딩 고려 가능
    
*** 타겟 비율을 기준으로 카테고리를 병합한 후 원핫 인코딩 vs 타겟 인코딩

    타겟 인코딩
    -> 타겟 인코딩은 카테고리 별 타겟의 평균값을 숫자(실수형)으로 반환
    모델이 연속형 변수로 인식하여 학습
    과적합의 위험이 있음
    회귀 모델을 사용할때 효과적
    
    
    병합 후 원핫인코딩
    -> 카테고리 병합 후 원핫인코딩은 이산형 변수로 변환. 범주형 변수로 유지.
        
/////////////////////////////

* 카테고리 수가 불균형할때 카테고리화 하는 방법

    희귀 카테고리 병합 - 전체 데이터에서 1-5%인 카테고리는 병합하는 것이 일반적
        타겟 변수와 강한 상관관계를 가진 카테고리를 잘못 병합하면 성능이 낮아질 수 있음
        
    상위 K개의 카테고리만 유지하고 나머지는 "others"로 병합 - 빈도순으로 상위 몇개의 주요 카테고리만 유지하고 싶을때
    
    언더샘플링, 오버샘플링 - *일반적으로는 재범주화 후에 샘플링
    
    타겟인코딩 - 타겟 변수의 평균을 계산하여 숫자로 변환
        회귀문제에서는 유용하지만, 분류 문제에서는 과적합의 위험이 있음

범주형 변수를 그룹화하거나 이진 변수 변환
예) "직업"을 고위험, 저위험으로 분류
특수한 경우:
범주형 데이터 개수가 너무 많으면: 특정 개수 이상만 유지하고 나머지는 "기타(Other)"로 묶음

---------------------------------------------------------------------------------------

(5) 데이터 불균형 처리 (Imbalanced Data Handling)

데이터의 기원에 따라서 샘플링에 따라 불균형을 처리. 만약 데이터가 자연적으로 불균형한 경우(사기 거래 탐지, 희귀 질병 진단)는 불균형을 유지하는 것이 나을수도 있음.

-> 특정 클래스가 적게 나오는 것이 정상적인 경우, 무리하게 균형을 맞추면 데이터의 본질이 왜곡될 수 있음

샘플링 외에 SMOTE, ADASYN 등의 기법을 통해 기존 데이터를 변형하여 생성
오버샘플링의 과적합 기술을 줄일 수 있음.

애초에 비율이 심한 불균형 데이터라면 가중치 조정, 앙상블 모델, 들의 방법이 더 효과적일 수 있음.

    오버샘플링
    
        소수 클래스의 샘플이 너무 적어서 학습이 어렵지만 데이터가 충분히 많아서 과적합 위험이 크지 않을때
        언더 샘플링을 하면 정보가 손실 될 가능성이 클 때
        
        전체 데이터 손실이 없으나 중복 샘플링은 과적합의 위험이 있음
        
        두 기법 모두 K-최근접 이웃(KNN) 개념을 활용하여 샘플을 생성
        SMOTE - 새로운 데이터를 생성(기존 데이터 포인트를 기반으로 새로운 데이터를 생성)
        ADASYN - 소수 클래스 내에서 다양한 샘플을 생성
        
        Borderline-SMOTE - 결정 경계(Decision Boundary)에 있는 샘플을 중심으로 데이터 생성, 경계 부분에서 더 나은 일반화를 할 수 있음
        소수 클래스가 경계에 몰려있는 경우 사용. 분류 경계를 명확하게 학습해야 하는 경우 사용
        
    언더샘플링
    
        다수 클래스가 너무 많아서 학습시 편향이 발생하는 경우
        데이터 양이 너무 많아 학습 속도가 느릴때
        
        NearMiss - 타겟 클래스와 가까운 샘플을 유지하면서 불균형 해소
        Tomek Links - 경계에 잇는 데이터를 제거하여 샘플 간격을 늘림
        
    오버샘플링과 언더샘플링을 혼합 -> "타겟 변수의 불균형"을 기준으로 샘플링이 진행
    
        SMOTETomek -  SMOTE로 소수 클래스를 보완하면서, Tomek Links를 사용하여 과적합을 방지
        
        from imblearn.combine import SMOTETomek

        X_resampled, y_resampled = SMOTETomek().fit_resample(X, y)
        
    불균형을 유지하며 모델 학습을 조정
    
        클래스 가중치 조정(Class Weighting) - 소수 클래스에 더 높은 가중치를 부여하여 균형을 맞춤
        
            class_weight="balanced"를 사용
        
        앙상블 모델 사용 (Ensemble Learning) - XGBoost, LightGBM 등의 부스팅 모델 활용
        
            scale_pos_weight를 통해 가중치 조정 가능
            
        *** 이상탐지 모델 사용(Anomaly Detection) - 소수 클래스가 극도로 적을떄 적합
        
            정상 데이터를 기반으로 패턴을 학습 후, 새 데이터가 기존 패턴에서 벗어나는지를 판단
            
            Isolation Forest - 데이터를 랜덤하게 분할하여 이상치가 쉽게 분리되는지 확인
            One-Class SVM - 정상 데이터의 결정경계를 학습하여 벗어나는 데이터를 이상치로 판단
            Elliptic Envelope - 정규분포를 가정하고 이상치를 탐지
     
/////////////////////////////

from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.05, random_state=42)  # 5%를 이상치로 감지
iso_forest.fit(X_train)
predictions = iso_forest.predict(X_test)

/////////////////////////////

    불균형 비율(IR) = 다수 클래스 샘플 개수 / 소수 클래스 샘플 개수
    
         IR ≥ 20 이면 이상탐지 모델을 고려
         IR ≥ 10~20 언더 샘플링 필요
         IR ≥ 5 이상 오버샘플링 필요(SMOTE, ADASYN)
         

타겟 변수의 클래스 비율이 불균형한 경우, SMOTE, ADASYN 같은 방법 사용
Undersampling도 있지만 데이터 손실 위험이 있음
특수한 경우:
불균형이 자연스러운 경우(사기탐지, 질병진단) → 특정 클래스만 샘플링해서 모델 평가하는 방법 고려

-> 사용할 데이터 불균형 처리법들에 대해서 모델링을 진행하고, F1 스코어, 재현율, 정밀도를 통해 처리법을 결정

분류 리포트의 활용 : 각각의 방법별로 리포트를 찍어 정밀도-재현율의 밸런스를 직관적으로 확인

---------------------------------------------------------------------------------------

(6) 파생변수 생성 (Feature Engineering)

    새로운 피처를 추가한 후, 모델의 성능이 실제로 향상되는지 확인하는 과정이 필수
    단순히 새로운 피처의 중요도(Feature Importance)만 확인하는 것이 아니라, 성능 변화도 함께 체크
    교차 검증(Cross Validation)을 사용하여 모델 성능을 비교하는 것이 효과적

기존 데이터를 조합하여 새로운 변수 생성 (월세/전용면적, 보증금/총층 등)
날짜 변수는 연도/월/요일 같은 새로운 변수로 분해하는 것이 일반적
특수한 경우:
도메인 지식이 중요한 경우 → 전문가의 의견 반영하여 주요 지표를 만들 수도 있음

*** 기본적으로 새로운 피처를 생성할 때마다 해당 피처가 모델 성능 향상에 기여하는지 확인하는 것이 중요

    날짜 데이터 변환
    
        연/월/일로 나누기 : 연도/월에 따라 변화하는 패턴이 있다면 효과적
            단순히 연/월을 나눈다고 해서 주기성을 반영할 수 있는 것은 아님
            특정 연도/월에 허위매물 비율이 급격히 변화한다면 유용
            
        특정 기간별로 카테고리화
            특정 기간이 중요한 경우 효과적
            허위매물 비율이 특정 시즌에 따라 차이가 난다면 유용할 수 있음
        
        사인/코사인 변환
            월/요일 데이터는 원형(주기적) 구조이므로, 단순 수치(1~12)로 모델에 넣으면 순서 정보가 반영되지 않음
            사인/코사인 변환을 하면 모델이 주기성을 더 잘 학습할 수 있음
            1월(1)과 12월(12)은 수치적으로 멀어 보이지만, 실제로는 가까운 시점 → 이를 해결하기 위해 변환 필요

---------------------------------------------------------------------------------------

(7) 다중공선성 문제 처리 -  공선성(Collinearity)은 특정 변수들이 서로 강한 상관관계를 가질 때 발생하는 문제

    회귀 모델(Linear Regression, Logistic Regression)에서 모델 성능 저하의 원인
    트리 기반 모델(RandomForest, XGBoost)에서는 공선성이 상대적으로 덜 문제가 됨

    문제 확인 : 피처간 상관관계 확인, VIF 계산
    
    공선성 문제 해결 방법 : 다중 공선성이 있는 경우, 공선성이 높은 피처를 제거하여 해결
    
        PCA(Principal Component Analysis) 적용 - 공선성이 있는 변수를 차원축소하여 새로운 변수로 변환
        
        정규화 또는 다항식 변환 후 변수 조정 - 로그변환이나 다항식 변환으로 조정 가능
        
        트리 기반 모델(RandomForest, XGBoost)은 공선성 문제의 영향을 거의 받지않으므로 모델 변경을 통해 해결 가능

공선성 분석은 보통 전반적인 데이터 전처리(결측치, 이상치 처리 등)가 끝난 후에 진행

데이터가 어느 정도 정리된 후, 피처 간의 관계를 분석하여 불필요한 변수를 제거하는 과정에서 수행하는 것이 효과적

-> 결측치가 많으면 공선성 분석이 왜곡될 수 있음. 이상치가 있으면 상관관계 분석이 부정확할 수 있음.

---------------------------------------------------------------------------------------

(8) 피처 선택(수 결정)

*** 피처 중요도를 통해 적절한 피처 수를 결정하는 방법 -> 원핫인코딩을 통해 증가하는 피처도 포함해서 고려해야 함

    일반적으로 상위 10-30개의 피처를 사용하는 것이 효과적
    -> 교차검증을 통해 최적의 개수를 찾아야 함

중요도 임계값(Threshold) 기반 선택

    피처 중요도가 0.01(1%) 이하인 변수는 제외하는 방식
    학습 시 거의 사용하지 않는 피처를 제거
    
*** Recursive Feature Elimination (RFE) 사용 - 모델이 중요하지 않은 피처를 자동으로 제거하여 최적의 피처 개수를 선택하는 기법

    초기 모델 학습: 모든 피처를 사용하여 모델 학습.
    피처 중요도 계산: 모델이 학습한 후, 중요도가 낮은 피처를 식별.
    가장 중요도가 낮은 피처 제거: 한 번에 하나(또는 여러 개)씩 제거.
    반복 진행: 최적의 피처 개수가 남을 때까지 반복.
    최종 선택: 남아있는 피처를 사용하여 최종 모델 학습.

---------------------------------------------------------------------------------------

* 특수한 상황

(1) 데이터 불균형 처리가 먼저 필요한 경우
이상치 제거 이후에 데이터 불균형이 심해진다면?
-> 데이터 증강(SMOTE)을 먼저 하고 이상치를 제거해야 함
-> 불균형을 해소한 후, 이상치 처리를 진행하는 것이 좋음

(2) 결측치 처리 전에 이상치를 먼저 다뤄야 하는 경우
결측치가 많지만 이상치가 영향을 줄 가능성이 있다면?
-> 이상치가 결측치를 유발했을 가능성이 있는 경우 -> 이상치 제거 후 결측치 처리 진행

(3) 이상치 처리 전에 스케일링을 해야 하는 경우
이상치가 너무 크거나 작은 값으로 왜곡되어 있다면?
-> 스케일링(Log 변환, Box-Cox)을 먼저 수행 후, 이상치 탐지 진행
-> 이상치가 정규화되면 IQR이나 Z-score 기반 탐지가 더 정확해짐

(4) 인코딩을 먼저 해야 하는 경우
결측치를 모델 기반으로 채울 때, 범주형 변수를 활용해야 한다면?
-> 범주형 변수를 Label Encoding 후 결측치를 모델로 채움
-> 예를 들어 RandomForestRegressor로 결측치를 채울 때, 범주형 변수를 Label Encoding한 뒤 사용해야 함


///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

*** 트리 기반 모델의 장점 (비선형 관계 학습시)

비선형 관계가 복잡한 경우 ex) 고차원 비선형 관계같은 경우 트리 모델은 분기(노드)를 통해 복잡한 관계를 자동으로 학습함

데이터 정규화(스케일링) 없이 사용할 때. 다른 선형 모델/ 신경망 모델은 스케일링이 필요하지만, 트리기반은 그런 전처리 없이 강한 성능을 띔

이상치에 강한 모델이 필요한 경우 -> 선형 모델은 이상치에 매우 취약함. 하지만 트리 기반 모델은 이상치의 영향을 덜 받음

변수간 상호작용이 중요한 경우 -> 선형 모델은 독립적인 피처만을 고려하지만, 트리 기반 모델은 변수 간 복합적인 상호작용을 자동으로 학습함

데이터의 크기가 크고 고차원인경우(피처수가 많을때) -> 피처 중요도를 통해 중요한 변수를 선별함. 랜텀포레스트나 XGBOOST 같은 트리 기반 모델은 다차원 데이터에서 강력한 성능을 보임.

*** 트리 기반 모델의 단점

연속적인 예측값이 필요한 경우. -> 계단 형태로 예측하기 때문에 가격 예측 등 연속적인 변화를 추적하는 문제에 적합하지 않음.

-> 대신 SVR, RNN 등을 사용해서 해결

데이터의 크기가 너무 작으면 오버피팅이 쉽게 발생할 수 있음

선형 모델의 경우 특정 변수가 결과에 얼마나 영향을 끼치는지 명확하게 설명 가능하지만, 트리기반 모델은 해석이 어렵다.

-> 피처 중요도나 SHAP값을 통해서 어느정도 해결 가능

딥러닝과 비교할때 학습 시간이 오래 걸림. 데이터 크기가 수백만 이상일때는 딥러닝이 더 적합할 수 있다.

고차원 데이터에서 희소한 정보를 다룰때 트리기반보다는 신경망이 더 적절할 수 있음.

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

